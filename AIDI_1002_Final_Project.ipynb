{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ad60af-6b1a-41c2-aa26-2e7da372b588",
   "metadata": {},
   "outputs": [],
   "source": [
    "#AIDI 1002: Final Project\n",
    "#Group Members: Gokul Shrivastav(200579267) & Omoririe Awolala()\n",
    "#Paper Link: https://arxiv.org/abs/1901.11196\n",
    "#GitHub:https://github.com/jasonwei20/eda_nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22189428-9e04-46b7-bd7a-4e54ebb2aaa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gokul Shrivastav\\eda_nlp\\code\\eda_nlp\\code\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'eda_nlp' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "#Cloning the repository\n",
    "!git clone https://github.com/jasonwei20/eda_nlp.git\n",
    "%cd eda_nlp/code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4a027633-713f-473b-b649-1908131f1945",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\anaconda\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: click in c:\\anaconda\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\anaconda\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\anaconda\\lib\\site-packages (from nltk) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in c:\\anaconda\\lib\\site-packages (from nltk) (4.66.5)\n",
      "Requirement already satisfied: colorama in c:\\anaconda\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\Gokul\n",
      "[nltk_data]     Shrivastav\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to C:\\Users\\Gokul\n",
      "[nltk_data]     Shrivastav\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\Gokul\n",
      "[nltk_data]     Shrivastav\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install nltk\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fcb5fb12-1a12-479f-93ef-60b6b523075a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gokul Shrivastav\\eda_nlp\\code\\eda_nlp\\code\\eda.py:177: SyntaxWarning: \"is not\" with 'str' literal. Did you mean \"!=\"?\n",
      "  words = [word for word in words if word is not '']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['semester want this i to end',\n",
       " 'i want this oddment semester to end',\n",
       " 'i this semester to end',\n",
       " 'i privation this semester to end',\n",
       " 'i want this semester to end ']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Running the model\n",
    "from eda import eda\n",
    "\n",
    "# Giving an example sentence\n",
    "sentence = \"I want this Semester to end \"\n",
    "\n",
    "# Generate the augmented sentences\n",
    "augmented_sentences = eda(sentence, num_aug=4)#setting how aggresive the augmentations are\n",
    "augmented_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fb25fa3c-99fd-4c93-acb8-231f08657388",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "canada is a great country\n",
      "canada personify is a great country\n",
      "canada is a great land\n",
      "canada is a great country\n",
      "the food for thought is amazing\n",
      "the food personify is amazing\n",
      "the food is amazing\n",
      "the food is amazing \n",
      "highly extremely recommend to everyone\n",
      "highly urge to everyone\n",
      "highly recommend everyone to\n",
      "highly recommend to everyone \n"
     ]
    }
   ],
   "source": [
    "#Giving multiple example sentences and also changing the augmentation attributes\n",
    "sentences = [\n",
    "    \"Canada is a Great country\",\n",
    "    \"The food is amazing.\",\n",
    "    \"Highly recommend to everyone.\",\n",
    "]\n",
    "\n",
    "aug_data = []\n",
    "for s in sentences:\n",
    "    aug_data.extend(eda(s, num_aug=3))\n",
    "\n",
    "for s in aug_data:\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f47d46-d9d2-4c7d-8401-64ea9d598e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can see above that formation of sentences is better with changed attributes\n",
    "#For making a contribution we will be eveluate the model on a pubicily available dataset on IMDB movie reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "87b159bc-d234-45bd-8020-98c4876c41c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dumb is as dumb does, in this thoroughly unint...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I dug out from my garage some old musicals and...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>After watching this movie I was honestly disap...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This movie was nominated for best picture but ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Just like Al Gore shook us up with his painful...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Unfortunately, this movie is so bad. The origi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Altered Species starts one Friday night in Los...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>'The Luzhin Defence' is a movie worthy of anyo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>This film's trailer interested me enough to wa...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>The only reason I watched this film was becaus...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  Dumb is as dumb does, in this thoroughly unint...      0\n",
       "1  I dug out from my garage some old musicals and...      1\n",
       "2  After watching this movie I was honestly disap...      0\n",
       "3  This movie was nominated for best picture but ...      1\n",
       "4  Just like Al Gore shook us up with his painful...      1\n",
       "5  Unfortunately, this movie is so bad. The origi...      0\n",
       "6  Altered Species starts one Friday night in Los...      0\n",
       "7  'The Luzhin Defence' is a movie worthy of anyo...      1\n",
       "8  This film's trailer interested me enough to wa...      0\n",
       "9  The only reason I watched this film was becaus...      0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Loading the new dataset and printing the first 10 values\n",
    "\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "dataset = load_dataset(\"imdb\")\n",
    "train_df = pd.DataFrame(dataset['train'])\n",
    "\n",
    "#Reducing dataset size for quicker experimentation\n",
    "train_df = train_df.sample(n=1000, random_state=42).reset_index(drop=True)\n",
    "\n",
    "train_df[['text', 'label']].head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "64a1a2ee-6cc6-44ed-8eb3-b05cb0cde8ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4563</th>\n",
       "      <td>this title seems more like a filming exercise ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>755</th>\n",
       "      <td>Steven Seagal....how could you be a part of su...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2402</th>\n",
       "      <td>this is another the curse of east asian cinema...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>863</th>\n",
       "      <td>I thought it was an extremely clever film. I w...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4429</th>\n",
       "      <td>if you like star wars trek cum see where they ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  label\n",
       "4563  this title seems more like a filming exercise ...      0\n",
       "755   Steven Seagal....how could you be a part of su...      0\n",
       "2402  this is another the curse of east asian cinema...      0\n",
       "863   I thought it was an extremely clever film. I w...      1\n",
       "4429  if you like star wars trek cum see where they ...      1"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Applying EDA to the training data\n",
    "\n",
    "augmented_texts = []\n",
    "augmented_labels = []\n",
    "\n",
    "for index, row in train_df.iterrows():\n",
    "    original_sentence = row['text']\n",
    "    label = row['label']\n",
    "    \n",
    "    #Applying EDA: generate 3 augmented versions per sentence\n",
    "    augmented_sentences = eda(original_sentence, num_aug=3)\n",
    "    \n",
    "    augmented_texts.extend(augmented_sentences)\n",
    "    augmented_labels.extend([label]*len(augmented_sentences))\n",
    "\n",
    "#Combining original and augmented data\n",
    "augmented_df = pd.DataFrame({\n",
    "    'text': train_df['text'].tolist() + augmented_texts,\n",
    "    'label': train_df['label'].tolist() + augmented_labels\n",
    "})\n",
    "\n",
    "augmented_df.sample(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a80cf79d-96f7-4f9f-a9fa-6ba5b1b07e84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data Performance:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.78      0.77       145\n",
      "           1       0.79      0.77      0.78       155\n",
      "\n",
      "    accuracy                           0.77       300\n",
      "   macro avg       0.77      0.77      0.77       300\n",
      "weighted avg       0.77      0.77      0.77       300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Training a classifier on the original and Augmented data\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "#Performing TF-IDF vectorization\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_features=3000)\n",
    "\n",
    "#Splitting original data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    train_df['text'], train_df['label'], test_size=0.3, random_state=42)\n",
    "\n",
    "X_train_vec = vectorizer.fit_transform(X_train)\n",
    "X_test_vec = vectorizer.transform(X_test)\n",
    "\n",
    "#Training on original data\n",
    "clf_orig = LogisticRegression(max_iter=200)\n",
    "clf_orig.fit(X_train_vec, y_train)\n",
    "\n",
    "y_pred_orig = clf_orig.predict(X_test_vec)\n",
    "print(\"Original Data Performance:\\n\", classification_report(y_test, y_pred_orig))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ca50a926-20cb-41e5-aacb-c070302518cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmented Data Performance:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      0.99       145\n",
      "           1       0.99      1.00      0.99       155\n",
      "\n",
      "    accuracy                           0.99       300\n",
      "   macro avg       0.99      0.99      0.99       300\n",
      "weighted avg       0.99      0.99      0.99       300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Splitting the augmented data\n",
    "X_aug_train, _, y_aug_train, _ = train_test_split(\n",
    "    augmented_df['text'], augmented_df['label'], test_size=0.2, random_state=42)\n",
    "\n",
    "X_aug_train_vec = vectorizer.fit_transform(X_aug_train)\n",
    "X_test_vec = vectorizer.transform(X_test)  \n",
    "\n",
    "#Training on augmented data\n",
    "clf_aug = LogisticRegression(max_iter=200)\n",
    "clf_aug.fit(X_aug_train_vec, y_aug_train)\n",
    "\n",
    "y_pred_aug = clf_aug.predict(X_test_vec)\n",
    "print(\"Augmented Data Performance:\\n\", classification_report(y_test, y_pred_aug))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ee7915bf-53a3-4278-8b6f-7979860d587d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv(\"original_data.csv\", index=False)\n",
    "augmented_df.to_csv(\"augmented_data.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fffbb998-3b41-47c2-936d-6df53cf887cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Contribution Summary\n",
    "#In this project, we investigated the performance of Jason Wei and Kai Zou's Easy Data Augmentation (EDA) technique on a new dataset,the IMDB movie reviews dataset, for binary sentiment classification.  The original EDA work did not use this dataset in its trials, therefore this is an extension of their methods to a different domain.\n",
    "#We replicated the original EDA augmentation workflow and used it on a subset of the IMDB training data.  We used a basic Logistic Regression classifier to train two models: one on the original dataset and one on the EDA-augmented dataset.  The model trained on the original data had an accuracy of 77%, whereas the model trained on the augmented data had a much greater accuracy of 99% on the identical test set.\n",
    "#This finding shows that EDA can significantly increase sentiment analysis performance by enriching the training data with syntactic and semantic changes.  While the performance increase is significant, we also identified possible concerns of overfitting or decreased data variety as a result of vigorous augmentation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
